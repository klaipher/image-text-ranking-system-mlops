{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f14e21109d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "model_package_group_name = f\"Cifar10ModelPackageGroupName\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413dbaed297ececc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585066bdf6455c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and upload CIFAR-10 data to S3\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "base_uri = f\"s3://{default_bucket}/cifar10\"\n",
    "\n",
    "# Ensure data directory exists\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download CIFAR-10 if not present locally\n",
    "cifar_local_path = data_dir / \"cifar-10-batches-py\"\n",
    "if not cifar_local_path.exists():\n",
    "    print(\"Downloading CIFAR-10 dataset...\")\n",
    "    cifar_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    tar_path = data_dir / \"cifar-10-python.tar.gz\"\n",
    "\n",
    "    # Download the tar file\n",
    "    urllib.request.urlretrieve(cifar_url, tar_path)\n",
    "    print(\"Downloaded CIFAR-10 tar file\")\n",
    "\n",
    "    # Extract the tar file\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(data_dir)\n",
    "    print(\"Extracted CIFAR-10 data\")\n",
    "\n",
    "    # Clean up the tar file\n",
    "    tar_path.unlink()\n",
    "\n",
    "# Verify the data exists and upload to S3\n",
    "if cifar_local_path.exists():\n",
    "    print(\"Uploading CIFAR-10 data to S3...\")\n",
    "    input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "        local_path=str(cifar_local_path),\n",
    "        desired_s3_uri=base_uri,\n",
    "    )\n",
    "    print(f\"CIFAR-10 data uploaded to: {input_data_uri}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"CIFAR-10 data not found even after download attempt. Please check your internet connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541593b107dc2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch inference data placeholder for CIFAR-10\n",
    "batch_data_uri = f\"{base_uri}/batch-inference-data\"\n",
    "print(f\"Batch data URI placeholder: {batch_data_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60297778ff4e16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "batch_data = ParameterString(\n",
    "    name=\"BatchData\",\n",
    "    default_value=batch_data_uri,\n",
    ")\n",
    "# New parameter for dataset size control\n",
    "dataset_percentage = ParameterFloat(name=\"DatasetPercentage\", default_value=0.5)\n",
    "# Updated threshold for classification accuracy instead of regression MSE  \n",
    "accuracy_threshold = ParameterFloat(name=\"AccuracyThreshold\", default_value=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fa4b2630c4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f0d0fc0abd17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# CIFAR-10 class names\n",
    "CIFAR10_CLASSES = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    \"\"\"Unpickle CIFAR-10 data files.\"\"\"\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "def load_cifar10_data(data_dir):\n",
    "    \"\"\"Load CIFAR-10 dataset from pickle files.\"\"\"\n",
    "    # Load training batches\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        batch_file = os.path.join(data_dir, f'data_batch_{i}')\n",
    "        batch_dict = unpickle(batch_file)\n",
    "        train_data.append(batch_dict[b'data'])\n",
    "        train_labels.extend(batch_dict[b'labels'])\n",
    "\n",
    "    # Combine all training data\n",
    "    train_data = np.vstack(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "\n",
    "    # Load test data\n",
    "    test_batch = unpickle(os.path.join(data_dir, 'test_batch'))\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = np.array(test_batch[b'labels'])\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "def process_cifar10_data(images, labels, dataset_percentage=1.0):\n",
    "    \"\"\"Process and reshape CIFAR-10 data.\"\"\"\n",
    "    # Reshape images from flat to 32x32x3\n",
    "    images = images.reshape(-1, 3, 32, 32)\n",
    "    # Convert from CIFAR format (CHW) to standard format (HWC)\n",
    "    images = images.transpose(0, 2, 3, 1)\n",
    "\n",
    "    # Apply dataset percentage sampling\n",
    "    if dataset_percentage < 1.0:\n",
    "        n_samples = len(images)\n",
    "        sample_size = max(1, int(n_samples * dataset_percentage))\n",
    "        indices = np.random.choice(n_samples, sample_size, replace=False)\n",
    "        images = images[indices]\n",
    "        labels = labels[indices]\n",
    "        print(f\"ðŸ“Š Sampled {len(images)} images ({dataset_percentage * 100:.1f}%) from {n_samples} total\")\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def save_data_splits(images, labels, base_dir, split_ratios=(0.7, 0.15, 0.15)):\n",
    "    \"\"\"Save data splits as numpy arrays.\"\"\"\n",
    "    n_samples = len(images)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(n_samples * split_ratios[0])\n",
    "    val_end = train_end + int(n_samples * split_ratios[1])\n",
    "\n",
    "    # Create splits\n",
    "    train_images = images[:train_end]\n",
    "    train_labels = labels[:train_end]\n",
    "\n",
    "    val_images = images[train_end:val_end]\n",
    "    val_labels = labels[train_end:val_end]\n",
    "\n",
    "    test_images = images[val_end:]\n",
    "    test_labels = labels[val_end:]\n",
    "\n",
    "    # Save splits\n",
    "    for split_name, split_images, split_labels in [\n",
    "        ('train', train_images, train_labels),\n",
    "        ('validation', val_images, val_labels),\n",
    "        ('test', test_images, test_labels)\n",
    "    ]:\n",
    "        split_dir = os.path.join(base_dir, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        np.save(os.path.join(split_dir, 'images.npy'), split_images)\n",
    "        np.save(os.path.join(split_dir, 'labels.npy'), split_labels)\n",
    "\n",
    "        print(f\"Saved {split_name}: {len(split_images)} samples\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dataset-percentage\", type=float, default=1.0,\n",
    "                        help=\"Percentage of dataset to use (0.1 = 10%, 1.0 = 100%)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    input_dir = os.path.join(base_dir, \"input\")\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    print(f\"Processing CIFAR-10 data with {args.dataset_percentage * 100:.1f}% of dataset...\")\n",
    "\n",
    "    # Load CIFAR-10 data\n",
    "    train_data, train_labels, test_data, test_labels = load_cifar10_data(input_dir)\n",
    "\n",
    "    # Combine all data for processing\n",
    "    all_images = np.vstack([train_data, test_data])\n",
    "    all_labels = np.hstack([train_labels, test_labels])\n",
    "\n",
    "    print(f\"Loaded {len(all_images)} total CIFAR-10 images\")\n",
    "\n",
    "    # Process data with dataset percentage\n",
    "    images, labels = process_cifar10_data(all_images, all_labels, args.dataset_percentage)\n",
    "\n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(len(images))\n",
    "    images = images[indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # Save processed splits\n",
    "    save_data_splits(images, labels, base_dir)\n",
    "\n",
    "    print(\"âœ… CIFAR-10 preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28cc548cdf68959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"1.2-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-abalone-process\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe096d992711613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"code/preprocessing.py\",\n",
    "    arguments=[\"--dataset-percentage\", Join(on=\"\", values=[dataset_percentage])],\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name=\"Cifar10Process\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d6361c01a0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "model_path = f\"s3://{default_bucket}/Cifar10Train\"\n",
    "\n",
    "# Use PyTorch for image classification\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"code\",\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version=\"2.7\",\n",
    "    py_version=\"py312\",\n",
    "    output_path=model_path,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    hyperparameters={\n",
    "        \"epochs\": 5,\n",
    "        \"batch-size\": 64,\n",
    "        \"learning-rate\": 0.001,\n",
    "        \"dataset-percentage\": Join(on=\"\", values=[dataset_percentage]),\n",
    "    }\n",
    ")\n",
    "\n",
    "train_args = pytorch_estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"application/x-npy\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"application/x-npy\",\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6db77e1909a28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/train.py\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ImprovedCNN(nn.Module):\n",
    "    \"\"\"Improved CNN for CIFAR-10 classification with modern architecture patterns.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.3):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "\n",
    "        # First block - more filters, batch norm\n",
    "        self.conv1a = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1a = nn.BatchNorm2d(64)\n",
    "        self.conv1b = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn1b = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Second block\n",
    "        self.conv2a = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2a = nn.BatchNorm2d(128)\n",
    "        self.conv2b = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.bn2b = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Third block\n",
    "        self.conv3a = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3a = nn.BatchNorm2d(256)\n",
    "        self.conv3b = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.bn3b = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Fourth block (optional - can comment out for simpler model)\n",
    "        self.conv4a = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4a = nn.BatchNorm2d(512)\n",
    "        self.conv4b = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.bn4b = nn.BatchNorm2d(512)\n",
    "\n",
    "        # Global average pooling instead of large FC layer\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Smaller FC layers\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier/He initialization.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "        x = F.relu(self.bn1b(self.conv1b(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Block 2\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Block 3\n",
    "        x = F.relu(self.bn3a(self.conv3a(x)))\n",
    "        x = F.relu(self.bn3b(self.conv3b(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Block 4\n",
    "        x = F.relu(self.bn4a(self.conv4a(x)))\n",
    "        x = F.relu(self.bn4b(self.conv4b(x)))\n",
    "\n",
    "        # Global average pooling\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # FC layers\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x  # Return logits directly (use CrossEntropyLoss)\n",
    "\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Load preprocessed CIFAR-10 data.\"\"\"\n",
    "    train_images = np.load(os.path.join(data_dir, \"train\", \"images.npy\"))\n",
    "    train_labels = np.load(os.path.join(data_dir, \"train\", \"labels.npy\"))\n",
    "\n",
    "    val_images = np.load(os.path.join(data_dir, \"validation\", \"images.npy\"))\n",
    "    val_labels = np.load(os.path.join(data_dir, \"validation\", \"labels.npy\"))\n",
    "\n",
    "    return train_images, train_labels, val_images, val_labels\n",
    "\n",
    "\n",
    "def create_data_loaders(train_images, train_labels, val_images, val_labels, batch_size):\n",
    "    \"\"\"Create PyTorch data loaders.\"\"\"\n",
    "    # Convert to tensors and normalize\n",
    "    train_images = torch.tensor(train_images, dtype=torch.float32) / 255.0\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "    val_images = torch.tensor(val_images, dtype=torch.float32) / 255.0\n",
    "    val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "    # Reshape to NCHW format for PyTorch\n",
    "    train_images = train_images.permute(0, 3, 1, 2)\n",
    "    val_images = val_images.permute(0, 3, 1, 2)\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = TensorDataset(train_images, train_labels)\n",
    "    val_dataset = TensorDataset(val_images, val_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def train_epoch(model, device, train_loader, optimizer, scheduler, epoch, criterion=None, grad_clip=None):\n",
    "    \"\"\"\n",
    "    Train for one epoch with improved CNN.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        device: Device to run on (cuda/cpu)\n",
    "        train_loader: DataLoader for training data\n",
    "        optimizer: Optimizer (Adam, SGD, etc.)\n",
    "        scheduler: Learning rate scheduler\n",
    "        epoch: Current epoch number\n",
    "        criterion: Loss function (defaults to CrossEntropyLoss)\n",
    "        grad_clip: Gradient clipping value (optional)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # Use CrossEntropyLoss for improved CNN (returns logits)\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss (CrossEntropyLoss expects logits, not log_softmax)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping (optional, helps with training stability)\n",
    "        if grad_clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        # Log progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            current_lr = scheduler.get_last_lr()[0] if scheduler else optimizer.param_groups[0]['lr']\n",
    "            logger.info(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                        f'({100. * batch_idx / len(train_loader):.0f}%)]\\t'\n",
    "                        f'Loss: {loss.item():.6f}\\tLR: {current_lr:.6f}')\n",
    "\n",
    "    # Update learning rate\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "\n",
    "    current_lr = scheduler.get_last_lr()[0] if scheduler else optimizer.param_groups[0]['lr']\n",
    "    logger.info(f'Train Epoch: {epoch} Complete - '\n",
    "                f'Average Loss: {avg_loss:.6f}, '\n",
    "                f'Accuracy: {accuracy:.2f}%, '\n",
    "                f'Learning Rate: {current_lr:.6f}')\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate(model, device, val_loader, criterion=None):\n",
    "    \"\"\"\n",
    "    Validate model on validation set.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        device: Device to run on (cuda/cpu)\n",
    "        val_loader: DataLoader for validation data\n",
    "        criterion: Loss function (defaults to CrossEntropyLoss)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = 100. * correct / len(val_loader.dataset)\n",
    "\n",
    "    logger.info(f'Validation - Average Loss: {avg_loss:.6f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def save_model(model, model_dir):\n",
    "    \"\"\"Save the trained model.\"\"\"\n",
    "    logger.info(\"Saving the model.\")\n",
    "    path = os.path.join(model_dir, \"model.pth\")\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load model for inference.\"\"\"\n",
    "    model = ImprovedCNN()\n",
    "    model_path = os.path.join(model_dir, \"model.pth\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # SageMaker specific arguments\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--validation\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\"))\n",
    "\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=128)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--dataset-percentage\", type=float, default=1.0)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Load data\n",
    "    logger.info(\"Loading data...\")\n",
    "    data_dir = args.train.replace(\"/train\", \"\") if args.train else \".\"\n",
    "    train_images, train_labels, val_images, val_labels = load_data(data_dir)\n",
    "\n",
    "    logger.info(f\"Train set: {len(train_images)} samples\")\n",
    "    logger.info(f\"Validation set: {len(val_images)} samples\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader, val_loader = create_data_loaders(\n",
    "        train_images, train_labels, val_images, val_labels, args.batch_size\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = ImprovedCNN().to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        weight_decay=1e-4,  # L2 regularization\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    # Training loop\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model=model,\n",
    "            device=device,\n",
    "            train_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            epoch=epoch,\n",
    "            criterion=criterion,\n",
    "            grad_clip=1.0\n",
    "        )\n",
    "        val_loss, val_acc = validate(model, device, val_loader, criterion)\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            if args.model_dir:\n",
    "                save_model(model, args.model_dir)\n",
    "\n",
    "    logger.info(f\"Training completed. Best validation accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb32ef430a1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"Cifar10Train\",\n",
    "    step_args=train_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da869688bf9c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/evaluation.py\n",
    "import json\n",
    "import pathlib\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"Simple CNN for CIFAR-10 classification - same as training script.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class ImprovedCNN(nn.Module):\n",
    "    \"\"\"Improved CNN for CIFAR-10 classification with modern architecture patterns.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.3):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "\n",
    "        # First block - more filters, batch norm\n",
    "        self.conv1a = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1a = nn.BatchNorm2d(64)\n",
    "        self.conv1b = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn1b = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Second block\n",
    "        self.conv2a = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2a = nn.BatchNorm2d(128)\n",
    "        self.conv2b = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.bn2b = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Third block\n",
    "        self.conv3a = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3a = nn.BatchNorm2d(256)\n",
    "        self.conv3b = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.bn3b = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Fourth block (optional - can comment out for simpler model)\n",
    "        self.conv4a = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4a = nn.BatchNorm2d(512)\n",
    "        self.conv4b = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.bn4b = nn.BatchNorm2d(512)\n",
    "\n",
    "        # Global average pooling instead of large FC layer\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Smaller FC layers\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier/He initialization.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "        x = F.relu(self.bn1b(self.conv1b(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Block 2\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Block 3\n",
    "        x = F.relu(self.bn3a(self.conv3a(x)))\n",
    "        x = F.relu(self.bn3b(self.conv3b(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Block 4\n",
    "        x = F.relu(self.bn4a(self.conv4a(x)))\n",
    "        x = F.relu(self.bn4b(self.conv4b(x)))\n",
    "\n",
    "        # Global average pooling\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # FC layers\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x  # Return logits directly (use CrossEntropyLoss)\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the trained PyTorch model.\"\"\"\n",
    "    model = ImprovedCNN()\n",
    "\n",
    "    # Extract model from tar.gz\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    # Load model state dict\n",
    "    model.load_state_dict(torch.load(\"model.pth\", map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_test_data(test_path):\n",
    "    \"\"\"Load test data.\"\"\"\n",
    "    test_images = np.load(f\"{test_path}/images.npy\")\n",
    "    test_labels = np.load(f\"{test_path}/labels.npy\")\n",
    "\n",
    "    return test_images, test_labels\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_images, test_labels, batch_size=128):\n",
    "    \"\"\"Evaluate the model on test data.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Convert to tensors and normalize\n",
    "    test_images = torch.tensor(test_images, dtype=torch.float32) / 255.0\n",
    "    test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "    # Reshape to NCHW format for PyTorch\n",
    "    test_images = test_images.permute(0, 3, 1, 2)\n",
    "\n",
    "    # Create data loader\n",
    "    test_dataset = TensorDataset(test_images, test_labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Evaluate\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target, reduction='sum')\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            pred = output.argmax(dim=1)\n",
    "            all_predictions.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    # Per-class metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average=None\n",
    "    )\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "\n",
    "    return {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'avg_loss': float(avg_loss),\n",
    "        'per_class_precision': precision_per_class.tolist(),\n",
    "        'per_class_recall': recall_per_class.tolist(),\n",
    "        'per_class_f1': f1_per_class.tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    test_path = \"/opt/ml/processing/test\"\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    test_images, test_labels = load_test_data(test_path)\n",
    "    print(f\"Test set: {len(test_images)} samples\")\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    metrics = evaluate_model(model, test_images, test_labels)\n",
    "\n",
    "    # Create evaluation report\n",
    "    report_dict = {\n",
    "        \"classification_metrics\": {\n",
    "            \"accuracy\": {\"value\": metrics['accuracy']},\n",
    "            \"precision\": {\"value\": metrics['precision']},\n",
    "            \"recall\": {\"value\": metrics['recall']},\n",
    "            \"f1_score\": {\"value\": metrics['f1_score']},\n",
    "            \"avg_loss\": {\"value\": metrics['avg_loss']},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if 'per_class_precision' in metrics:\n",
    "        report_dict[\"additional_metrics\"] = {\n",
    "            \"per_class_precision\": [float(x) for x in metrics['per_class_precision']],\n",
    "            \"per_class_recall\": [float(x) for x in metrics['per_class_recall']],\n",
    "            \"per_class_f1_score\": [float(x) for x in metrics['per_class_f1']]\n",
    "        }\n",
    "\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "    # Save evaluation results\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        json.dump(report_dict, f, indent=2)\n",
    "\n",
    "    print(f\"Evaluation complete! Results saved to {evaluation_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c50b839bd6d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "\n",
    "script_eval = PyTorchProcessor(\n",
    "    framework_version=\"2.7\",\n",
    "    py_version=\"py312\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-cifar10-eval\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "eval_args = script_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"code/evaluation.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b940e6f61a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"Cifar10Eval\",\n",
    "    step_args=eval_args,\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e1b99856ff1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"code\",\n",
    "    framework_version=\"2.6.0\",\n",
    "    py_version=\"py312\",\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "    name=\"Cifar10CreateModel\",\n",
    "    step_args=model.create(instance_type=\"ml.m5.large\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb9f30d1285b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "register_args = model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "step_register = ModelStep(name=\"Cifar10RegisterModel\", step_args=register_args, depends_on=[step_eval] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0bb885a27d0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "step_fail = FailStep(\n",
    "    name=\"Cifar10AccuracyFail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to accuracy <\", accuracy_threshold]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8972cf63f53335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"classification_metrics.accuracy.value\",\n",
    "    ),\n",
    "    right=accuracy_threshold,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"Cifar10AccuracyCond\",\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[step_register, step_create_model],\n",
    "    else_steps=[step_fail],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d5afa1ad2a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"Cifar10Pipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        batch_data,\n",
    "        dataset_percentage,\n",
    "        accuracy_threshold,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1c9b978be0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6bdbd140189c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40ea9033ba7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818f3e2d5d028aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e87536b41d2f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f53935e48f7f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\n",
    "    \"{}/evaluation.json\".format(\n",
    "        step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "    )\n",
    ")\n",
    "pprint(json.loads(evaluation_json))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
